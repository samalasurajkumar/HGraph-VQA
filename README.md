# HGraph-VQA
Abstract: The growing demand for immersive media has made perceptual quality assessment a key challenge in 360-degree video applications. Despite notable progress in video quality assessment (VQA), most existing methods fail to effectively capture the complex spatial–temporal dependencies and varying visual importance that characterize omnidirectional content. To address these challenges, this paper presents HGraph-VQA, a hierarchical graph learning framework for blind 360° video quality prediction. Each video is decomposed into multiple viewports, from which four complementary cues—visual appearance, semantic context, 360°-specific artifacts, and importance weighting—are extracted and fused into a unified 1291-dimensional descriptor. Spatial graphs model intra-frame dependencies among perceptually correlated viewports, while temporal graphs capture motion continuity and cross-frame coherence. The hierarchical architecture leverages graph attention networks for adaptive feature aggregation and employs a lightweight regression head to estimate mean opinion scores (MOS). Experiments on the VQA-ODV benchmark demonstrate that HGraph-VQA achieves a PLCC of 0.9594 and SRCC of 0.9683, outperforming recent state-of-the-art approaches. These results confirm that integrating hierarchical graph reasoning with importance-aware feature fusion provides a more accurate and interpretable understanding of human perception in 360° visual environments.
